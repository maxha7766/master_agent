-- ============================================
-- Tabular Data System - Database Schema
-- Migration: 004_tabular_data_system
-- Created: 2025-10-29
-- ============================================
-- Extends existing documents table to support CSV/Excel files
-- Adds row-level JSONB storage for tabular data queries

-- ============================================
-- 1. EXTEND EXISTING TABLES
-- ============================================

-- Extend documents table with tabular-specific fields
-- (documents table already exists from migration 001)

-- Add user description field (helps AI understand context)
ALTER TABLE documents
ADD COLUMN IF NOT EXISTS user_description TEXT;

-- Add AI-generated semantic schema (for CSV/Excel files)
-- Contains: columns (name, type, nullable, stats), primary_key, summary
ALTER TABLE documents
ADD COLUMN IF NOT EXISTS semantic_schema JSONB;

-- Add row count (for tabular data)
ALTER TABLE documents
ADD COLUMN IF NOT EXISTS row_count INTEGER DEFAULT 0;

-- Add column count (for tabular data)
ALTER TABLE documents
ADD COLUMN IF NOT EXISTS column_count INTEGER DEFAULT 0;

-- Add data quality score (0.0-1.0)
ALTER TABLE documents
ADD COLUMN IF NOT EXISTS data_quality_score NUMERIC(3,2);

-- Add content hash for deduplication
ALTER TABLE documents
ADD COLUMN IF NOT EXISTS content_hash VARCHAR(64);

-- ============================================
-- 2. NEW TABLES
-- ============================================

-- Table: document_data
-- Stores actual row-level data from CSV/Excel in JSONB format
-- Each row from the CSV/Excel becomes one record here
CREATE TABLE document_data (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,

  -- Row Data (ALL columns stored as JSONB key-value pairs)
  -- Example: {"customer_id": "1001", "amount": "250.50", "date": "2024-10-15"}
  row_data JSONB NOT NULL,

  -- Row position in original file (for ordering)
  row_index INTEGER NOT NULL,

  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() NOT NULL,

  -- Ensure unique rows per document
  UNIQUE(document_id, row_index)
);

-- Table: query_history
-- Tracks natural language queries and generated SQL for learning/debugging
CREATE TABLE query_history (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID NOT NULL REFERENCES auth.users(id) ON DELETE CASCADE,

  -- Query Details
  natural_query TEXT NOT NULL, -- User's original question
  generated_sql TEXT NOT NULL, -- SQL generated by AI
  documents_used UUID[] NOT NULL, -- Which document IDs were queried

  -- Results
  result_count INTEGER, -- Number of rows returned
  success BOOLEAN NOT NULL,
  error_message TEXT,
  execution_time_ms INTEGER,

  -- Metadata (for learning patterns)
  intent_type VARCHAR(50), -- 'SELECT', 'AGGREGATE', 'JOIN', etc.
  confidence_score NUMERIC(3,2),

  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW() NOT NULL
);

-- ============================================
-- 2. INDEXES
-- ============================================

-- document_metadata indexes
CREATE INDEX idx_doc_meta_user ON document_metadata(user_id, created_at DESC);
CREATE INDEX idx_doc_meta_status ON document_metadata(status) WHERE status = 'completed';
CREATE INDEX idx_doc_meta_created ON document_metadata(created_at DESC);

-- document_data indexes (CRITICAL FOR PERFORMANCE)
CREATE INDEX idx_doc_data_document ON document_data(document_id);
CREATE INDEX idx_doc_data_row_index ON document_data(document_id, row_index);

-- GIN index for fast JSONB queries (enables queries on any column)
CREATE INDEX idx_doc_data_jsonb ON document_data USING GIN(row_data);

-- query_history indexes
CREATE INDEX idx_query_history_user ON query_history(user_id, created_at DESC);
CREATE INDEX idx_query_history_docs ON query_history USING GIN(documents_used);
CREATE INDEX idx_query_history_success ON query_history(success, created_at DESC);

-- ============================================
-- 3. ROW LEVEL SECURITY (RLS)
-- ============================================

-- Enable RLS on new tables
ALTER TABLE document_data ENABLE ROW LEVEL SECURITY;
ALTER TABLE query_history ENABLE ROW LEVEL SECURITY;

-- document_data RLS policies
CREATE POLICY "Users can view own document data"
  ON document_data FOR SELECT
  USING (
    document_id IN (
      SELECT id FROM documents WHERE user_id = auth.uid()
    )
  );

CREATE POLICY "Users can insert own document data"
  ON document_data FOR INSERT
  WITH CHECK (
    document_id IN (
      SELECT id FROM documents WHERE user_id = auth.uid()
    )
  );

CREATE POLICY "Users can delete own document data"
  ON document_data FOR DELETE
  USING (
    document_id IN (
      SELECT id FROM documents WHERE user_id = auth.uid()
    )
  );

-- query_history RLS policies
CREATE POLICY "Users can view own query history"
  ON query_history FOR SELECT
  USING (auth.uid() = user_id);

CREATE POLICY "Users can insert own query history"
  ON query_history FOR INSERT
  WITH CHECK (auth.uid() = user_id);

CREATE POLICY "Users can delete own query history"
  ON query_history FOR DELETE
  USING (auth.uid() = user_id);

-- ============================================
-- 4. UPDATED_AT TRIGGER
-- ============================================

-- Trigger to auto-update updated_at timestamp
CREATE TRIGGER update_document_metadata_updated_at
  BEFORE UPDATE ON document_metadata
  FOR EACH ROW
  EXECUTE FUNCTION update_updated_at_column();

-- ============================================
-- 5. HELPER FUNCTIONS
-- ============================================

-- Function to safely execute SQL against document_data
-- Used by the SQL execution agent
CREATE OR REPLACE FUNCTION execute_tabular_query(
  p_user_id UUID,
  p_query TEXT
)
RETURNS JSONB
LANGUAGE plpgsql
SECURITY DEFINER
AS $$
DECLARE
  v_result JSONB;
  v_start_time TIMESTAMPTZ;
  v_execution_time INTEGER;
BEGIN
  -- Validate user has access
  IF p_user_id IS NULL OR p_user_id != auth.uid() THEN
    RAISE EXCEPTION 'Unauthorized access';
  END IF;

  -- Validate query is read-only (basic check)
  IF p_query ~* '\b(INSERT|UPDATE|DELETE|DROP|ALTER|CREATE|TRUNCATE|GRANT|REVOKE)\b' THEN
    RAISE EXCEPTION 'Only SELECT queries are allowed';
  END IF;

  -- Execute query with timing
  v_start_time := clock_timestamp();

  EXECUTE format('SELECT jsonb_agg(row_to_json(t)) FROM (%s) t', p_query)
  INTO v_result;

  v_execution_time := EXTRACT(MILLISECONDS FROM clock_timestamp() - v_start_time)::INTEGER;

  -- Return results with metadata
  RETURN jsonb_build_object(
    'success', true,
    'data', COALESCE(v_result, '[]'::jsonb),
    'execution_time_ms', v_execution_time,
    'row_count', jsonb_array_length(COALESCE(v_result, '[]'::jsonb))
  );

EXCEPTION WHEN OTHERS THEN
  RETURN jsonb_build_object(
    'success', false,
    'error', SQLERRM,
    'error_code', SQLSTATE
  );
END;
$$;

-- Function to get document schema by ID
CREATE OR REPLACE FUNCTION get_document_schema(p_document_id UUID)
RETURNS JSONB
LANGUAGE plpgsql
SECURITY DEFINER
AS $$
DECLARE
  v_schema JSONB;
BEGIN
  SELECT semantic_schema INTO v_schema
  FROM document_metadata
  WHERE id = p_document_id
    AND user_id = auth.uid()
    AND status = 'completed';

  IF v_schema IS NULL THEN
    RAISE EXCEPTION 'Document not found or not accessible';
  END IF;

  RETURN v_schema;
END;
$$;

-- ============================================
-- 6. COMMENTS
-- ============================================

COMMENT ON TABLE document_data IS 'Stores row-level data from CSV/Excel files in JSONB format for flexible querying';
COMMENT ON TABLE query_history IS 'Tracks natural language queries and generated SQL for learning and debugging';

COMMENT ON COLUMN documents.semantic_schema IS 'AI-generated schema with column types, descriptions, statistics, and relationships (for CSV/Excel files)';
COMMENT ON COLUMN documents.user_description IS 'User-provided context about the data (e.g., "Q4 sales from Northeast region")';
COMMENT ON COLUMN document_data.row_data IS 'JSONB object containing all column values for this row';

-- ============================================
-- 7. EXAMPLE QUERIES
-- ============================================

/*
-- Insert document metadata
INSERT INTO document_metadata (user_id, filename, file_type, file_size, semantic_schema, row_count, column_count, status)
VALUES (
  auth.uid(),
  'sales_q4_2024.csv',
  'csv',
  1024000,
  '{"columns": [{"name": "customer_id", "type": "integer"}, {"name": "amount", "type": "numeric"}], "summary": "Sales data"}'::jsonb,
  1500,
  5,
  'completed'
);

-- Insert row data
INSERT INTO document_data (document_id, row_data, row_index)
VALUES (
  'uuid-here',
  '{"customer_id": "1001", "amount": "250.50", "date": "2024-10-15"}'::jsonb,
  0
);

-- Query data (example)
SELECT
  row_data->>'customer_id' as customer,
  SUM((row_data->>'amount')::numeric) as total
FROM document_data
WHERE document_id = 'uuid-here'
GROUP BY row_data->>'customer_id'
ORDER BY total DESC
LIMIT 10;

-- Get schema
SELECT get_document_schema('uuid-here');

-- Execute safe query
SELECT execute_tabular_query(
  auth.uid(),
  'SELECT row_data->''customer_id'' as customer FROM document_data WHERE document_id = ''uuid-here'' LIMIT 10'
);
*/
